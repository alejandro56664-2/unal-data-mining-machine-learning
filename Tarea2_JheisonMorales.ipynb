{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea2_JheisonMorales.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qwkoMHGnBs6K",
        "DHegV-5OEAhR",
        "A8T68OdKUwY3",
        "eTBiKWSEp_41",
        "NMI5UhSkp8b-",
        "rkx4_ahYAUWW",
        "n0oMYpikWAJK"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQHVPQWdSVUkzMlXpVw84N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alejandro56664-adl/unal-data-mining/blob/main/Tarea2_JheisonMorales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ah0dV4vopr"
      },
      "source": [
        "\n",
        "# Minería de Datos - 2021-I\n",
        "## Tarea 2: Preprocesamiento\n",
        "\n",
        "![logo_unal](http://seeklogo.com/images/U/Universidad_Nacional_de_Colombia_-_Sede_Bogot_and__225_-logo-A05EAD6D0F-seeklogo.com.png)\n",
        "\n",
        "Presentado por:\n",
        "- [Jheison Alejandro Morales Vásquez](mailto:jhmoralesva@unal.edu.co)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-iCEfmX0gk0"
      },
      "source": [
        "# Python\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from numpy import mean, absolute \n",
        "import os\n",
        "import math\n",
        "import io\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import trim_mean\n",
        "from scipy.spatial import distance\n",
        "from scipy.stats import entropy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBp6hRVz5YC"
      },
      "source": [
        "## 1.Dado un conjunto de datos de una dimensión \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQIkTNQm0JkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e470f73b-b160-448b-c112-cac9b7b9eff3"
      },
      "source": [
        "X = np.array([-5.0, 23.0, 17.6, 7.23, 1.11])\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-5.   23.   17.6   7.23  1.11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xAKkS-j082D"
      },
      "source": [
        " normalizar usando:\n",
        "- Min_max normalizacion en el Intervalo [0,1]\n",
        "- Min_max normalizacion en el Intervalo [-1,1]\n",
        "- Desviación estandar\n",
        "- Escala decimal en el intervalo [-1,1]\n",
        "- Comparar los resultados y discutir ventajas y desventajas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS5queng-Sh5"
      },
      "source": [
        "#  Min_max normalizacion implementada\n",
        "def norm_min_max(p_array, scale=(0,1)):\n",
        "  p_array_min = np.amin(p_array)\n",
        "  diff_max_min = np.amax(p_array) - p_array_min\n",
        "  norm = (p_array - p_array_min)/diff_max_min\n",
        "  return norm* (scale[1] - scale[0]) + scale[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF1m-sfh0yz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b13abb-18ee-4dad-e88a-b6363507c5d6"
      },
      "source": [
        "# 1. Min_max normalizacion en el Intervalo [0,1] implementada\n",
        "\n",
        "X_norm_min_max_own = norm_min_max(X)\n",
        "print(X_norm_min_max_own)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         1.         0.80714286 0.43678571 0.21821429]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImNQE4l31n_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf47f6b2-27fe-43b6-85dc-f91ab125c21f"
      },
      "source": [
        "# obtenido de: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Se convierte en una matriz de 2 dimensiones donde (n_samples, n_features), en \n",
        "# esta caso 5 muestras y 1 caracteristica\n",
        "X_2d = np.reshape(X, (-1, X.size)).T\n",
        "print('Matriz 2d formato (n_samples, n_features):')\n",
        "print(X_2d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz 2d formato (n_samples, n_features):\n",
            "[[-5.  ]\n",
            " [23.  ]\n",
            " [17.6 ]\n",
            " [ 7.23]\n",
            " [ 1.11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWN741DT41Vc"
      },
      "source": [
        "# 1. Min_max normalizacion en el Intervalo [0,1] scikit-learn\n",
        "X_norm_min_max =  preprocessing.MinMaxScaler().fit_transform(X_2d)\n",
        "print(X_norm_min_max)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6c9MDB4-BA7"
      },
      "source": [
        "# 2. Min_max normalizacion en el Intervalo [-1,1] implementada\n",
        "X_norm_min_max_own_2 = norm_min_max(X, scale=(-1,1))\n",
        "print(X_norm_min_max_own_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUXcpRT3-sEC"
      },
      "source": [
        "# 2. Min_max normalizacion en el Intervalo [-1,1] scikit-learn\n",
        "X_norm_min_max_2 =  preprocessing.MinMaxScaler(feature_range=(-1,1)).fit_transform(X_2d)\n",
        "print(X_norm_min_max_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytda3OCrAZDh"
      },
      "source": [
        "# Desviación estandar\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.std.html\n",
        "def norm_desviacion_estandar(p_array):\n",
        "  return (p_array - p_array.mean())/p_array.std()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOGnBRL8BDMH"
      },
      "source": [
        "norm_desviacion_estandar(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR9U-BACxMKJ"
      },
      "source": [
        "Para la normalización basado en la escala decimal:\n",
        "\n",
        "- Encontrar el número mas grande en el conjunto de datos\n",
        "- Contar el número de digitos _j_ del número mas grande.Count the number\n",
        "- Dividir cada número por 10^j"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0z1Y3I7AWq8"
      },
      "source": [
        "# Escala decimal en el intervalo [-1,1]\n",
        "def norm_dec(p_array):\n",
        "  # Encontrar el número mas grande en el conjunto de datos y\n",
        "  max_value = p_array.max()\n",
        "  # Contar el número de digitos j del número mas grande.Count the number\n",
        "  j = np.ceil(np.log10(max_value))\n",
        "  #Dividir cada número por 10^j\n",
        "  norm = p_array/10**j\n",
        "  return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJlRi6odymQ5"
      },
      "source": [
        "\n",
        "X_norm_dec = norm_dec(X)\n",
        "print(X_norm_dec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njawcomj0HAk"
      },
      "source": [
        "#para llevarlo al rango -1,1 aplicamos min_max\n",
        "X_norm_dec_min_max = norm_min_max(X_norm_dec, (-1,1))\n",
        "print(X_norm_dec_min_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwCz8YFK_7Pi"
      },
      "source": [
        "### Ventajas y desventajas\n",
        "\n",
        "- Min_max normalizacion, es útil porque permite comprimir en un rango especifico los datos y reduce la suceptibilidad a las escalas.\n",
        "\n",
        "- Desviación estandar: es útil para identificar objetos extraños (outliers) dentro de un conjunto de datos usando solamente los estadisticos.\n",
        "\n",
        "- Escala decimal en el intervalo: Permite reducir de cierta manera la 'distancia' entre objetos cuyas escalas difieren mucho en orden de magnitud. Depende de la aplicación en donde esta distancia se pueda evitar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwkoMHGnBs6K"
      },
      "source": [
        "## 2.Dado un conjunto de datos de 4 dimensiones con valores perdidos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjYqcX3YY7n0"
      },
      "source": [
        "| I1 | I2 | I3 | I4 |\n",
        "| -- | -- | -- | -- |\n",
        "| 0  | 1  | 1  | 2  |\n",
        "| 2  | 1  | ?  | 1  |\n",
        "| 1  | ?  | ?  | 0  |\n",
        "| ?  | 2  | 1  | ?  |\n",
        "| 2  | 2  | 1  | 0  |\n",
        "\n",
        "\n",
        "- Dado que el dominio para todos los atributos es [0,1,2] ¿Cuál debe ser el número de ejemplos “artificiales” si los valores perdidos son interpretados como “no importa el valor” y ellos son remplazados con todos los posibles valores para su dominio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkIzVUTm1HYn"
      },
      "source": [
        "df_perdidos = pd.DataFrame(np.array([[0,\t1,\t1,\t2], \n",
        "                                      [2,\t1,\t'?',\t1], \n",
        "                                      [1,\t'?', '?',\t0],\n",
        "                                      ['?',\t2,\t1,\t'?'],\n",
        "                                      [2,\t2,\t1,\t0]] ),\n",
        "                            columns=['I1', 'I2', 'I3', 'I4'])\n",
        "\n",
        "df_perdidos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdpbid5XXLhz"
      },
      "source": [
        "def generate_syntethic_rows(df, col, domain):\n",
        "  df_new = pd.DataFrame()\n",
        "  for index, row in df.iterrows():\n",
        "    row_dict = row.to_dict()\n",
        "    if row_dict[col] == '?':\n",
        "      for d in domain: #dominio de los datos\n",
        "        row_dict[col] = d\n",
        "        df_new = df_new.append(row_dict, ignore_index=True)\n",
        "    else:\n",
        "      df_new = df_new.append(row_dict, ignore_index=True)\n",
        "  return df_new "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABYEjtI6ZGH3"
      },
      "source": [
        "df_aux = df_perdidos\n",
        "for col in df_perdidos:\n",
        "  df_aux = generate_syntethic_rows(df_aux, col, ['0','1','2'])\n",
        "\n",
        "df_aux"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT0_sxO6Z4S2"
      },
      "source": [
        "print(df_aux.shape[0] - df_perdidos.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6sYvvwpZ11O"
      },
      "source": [
        "R/: Sería necesario agregar 18\n",
        "\n",
        "- La segunda instancia sería reemplazada por 3 nuevas instancias.\n",
        "- La tercera y cuarta instancia por 9.\n",
        "\n",
        "```nuevas_instancias = (3 - 1) + (3^2 - 1)*2 = 2 + 8*2 = 2 + 16 = 18```\n",
        "\n",
        "- ¿Cuál otro método utilizaría para remplazar los valores pérdidos?\n",
        "\n",
        "R/: Reemplazar cada valor perdido por la moda de la columna a la que pertence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHegV-5OEAhR"
      },
      "source": [
        "## 3.El número de hijos de diferentes pacientes es dado por el siguiente vector:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Yxq6MhBweB"
      },
      "source": [
        "num_hijos = np.array([3,1,0,2,7,3,6,4,-2,0,0,10,15,6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwir3JXSEHj5"
      },
      "source": [
        "- Encontrar “outliers” usando parámetros estadisticos estándar: media y varianza\n",
        "- Si el umbral cambia de ±3 desviaciones estandar a ±2 desviaciones estandar , ¿Cuál “outlier adicional” se encuentra?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr5HJwQwEf81"
      },
      "source": [
        "import statistics\n",
        "\n",
        "mediana = statistics.median(num_hijos)\n",
        "media = num_hijos.mean()\n",
        "var   = num_hijos.var()\n",
        "std   = num_hijos.std()\n",
        "\n",
        "\n",
        "print('Media: ' + str(media))\n",
        "print('Mediana: ' + str(mediana))\n",
        "print('Varianza: ' + str(var))\n",
        "print('Desviación: ' + str(std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-vTuhZnEctk"
      },
      "source": [
        "\n",
        "def get_outliers(p_array, th):\n",
        "  std_sample = p_array.std()/math.sqrt(len(p_array))\n",
        "  z_p_array = (p_array - p_array.mean())/std_sample\n",
        "  outliers = np.array([x for x in z_p_array if (x < -th).sum() or (x >= th).sum()])\n",
        "  #devolvemos la transformación\n",
        "  outliers = outliers*std_sample + p_array.mean()\n",
        "  return outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EG1GXS6Et2E"
      },
      "source": [
        "outliers = get_outliers(num_hijos, th=3)\n",
        "outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06LIFC90E4r0"
      },
      "source": [
        "outliers = get_outliers(num_hijos, th=2)\n",
        "outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNYV5mqRJbtM"
      },
      "source": [
        "Con 3 sigmas es suficiente para detectar valores atipicos, con dos parece que se incluyen valores que pueden ser considerados normales (como 1, por ejemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNP4AT6ZJoYn"
      },
      "source": [
        "## 4.Dado un conjunto de tres dimensiones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9KQGccFZHfZ"
      },
      "source": [
        "\n",
        "a) Describir el procedimiento e interpretar los resultados de detección de outliers basado en la media y varianza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyuZTGTTJwAP"
      },
      "source": [
        "vector_3d = np.array([[1,2,0],[3,1,4],[2,1,5],[0,1,6],[2,4,3],[4,4,2],[5,2,1],[7,7,7],[0,0,0],[3,3,3]])\n",
        "print(vector_3d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzdsuWUOKhIt"
      },
      "source": [
        "mediana = [statistics.median(vector_3d[:,0]), \n",
        "           statistics.median(vector_3d[:,1]), \n",
        "           statistics.median(vector_3d[:,2])]\n",
        "           \n",
        "media = vector_3d.mean(axis=0)\n",
        "var   = vector_3d.var(axis=0)\n",
        "std   = vector_3d.std(axis=0)\n",
        "\n",
        "print('Mediana: ' + str(mediana))\n",
        "print('Media: ' + str(media))\n",
        "print('Varianza: ' + str(var))\n",
        "print('Desviación: ' + str(std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsT2NqfvO5if"
      },
      "source": [
        "outliers = get_outliers(vector_3d, 2)\n",
        "outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq4R0cz8UOTZ"
      },
      "source": [
        "outliers = get_outliers(vector_3d, 3)\n",
        "outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDJUdVFHUWiE"
      },
      "source": [
        "### interpretación, procedimiento comentario\n",
        "\n",
        "Para descartar outliers se puede usar el [z-score](https://en.wikipedia.org/wiki/Standard_score), por ejemplo descartar aquellos que tengan \n",
        "\n",
        "```Z>=3 && Z<-3```\n",
        "\n",
        "\n",
        "Explicación z-score\n",
        "![diagrama explicativo](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/The_Normal_Distribution.svg/2560px-The_Normal_Distribution.svg.png \"Distribución normal\")\n",
        "\n",
        "Para el análisis se tuvieron en cuenta todas las columnas, suponiendo que el dominio de los datos es igual para todas las caracteristicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8T68OdKUwY3"
      },
      "source": [
        "## 5.En Weka cargar el conjunto de datos iris.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Lwk5lgZMs3"
      },
      "source": [
        "En este caso se realizó el mismo procedimiento usando python.\n",
        "\n",
        "- Eliminar manualmente valores (15%) en sus atributos, para simular valores perdidos. Luego aplicar varios métodos que estan en weka para remplazar esos valores perdidos. Discutir las diferencias entre el valor real y el que valor que lo remplaza, y las diferencias entre los métodos.\n",
        "- Normalizar usando varios metodos.\n",
        "- Discretizar usando varios métodos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTBiKWSEp_41"
      },
      "source": [
        "### 5.1 Eliminar manualmente valores (15%) en sus atributos, para simular valores perdidos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdIkh_LL1EOy"
      },
      "source": [
        "#carga de los datos de iris\n",
        "df_iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names=['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
        "df_iris.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yubFaDKx2M46"
      },
      "source": [
        "# eliminación del 15% de los datos\n",
        "\n",
        "# calculamos todos los valores en el dataset\n",
        "def calculate_data_to_delete(df, percent):\n",
        "  rows,cols = df.shape\n",
        "  total_data = rows*cols #no tenemos en cuenta la clase\n",
        "  return math.floor(total_data*percent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4uyP7D9iUug"
      },
      "source": [
        "import random\n",
        "def remove_data_in_row(p_row, max_to_delete):\n",
        "  count_data_deleted = 0\n",
        "  row = p_row.copy()\n",
        "  for k,v in row.items():\n",
        "    if random.uniform(0,1) > 0.5:\n",
        "      row[k] = float('NaN')\n",
        "      count_data_deleted = count_data_deleted +1\n",
        "      if count_data_deleted >= max_to_delete:\n",
        "        break\n",
        "  return row,count_data_deleted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbTVpFfJhM8a"
      },
      "source": [
        "row_test_dirty,_ = remove_data_in_row({'c1': 1.0, 'c2': 2.0, 'c3': 3.0, 'c4': 4.0}, max_to_delete=2)\n",
        "row_test_dirty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsGsEpaLfB7_"
      },
      "source": [
        "def remove_n_values(df, n):\n",
        "  df_new = pd.DataFrame()\n",
        "  max_to_delete_per_row = 2\n",
        "  count_data_deleted = 0\n",
        "  for index, row in df.iterrows():\n",
        "    row_dirty = row.to_dict()\n",
        "    #si todavia podemos quitar datos y por azar la siguiente columna la vamos ensuciar\n",
        "    if count_data_deleted <= n and random.uniform(0,1) > 0.5:\n",
        "      row_dirty, data_deleted = remove_data_in_row(row_dirty, max_to_delete_per_row)\n",
        "      count_data_deleted = count_data_deleted + data_deleted\n",
        "    df_new = df_new.append(row_dirty, ignore_index=True)\n",
        "  return df_new\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYuagHjtmb4H"
      },
      "source": [
        "df_test = remove_n_values(df_iris.iloc[1:5,:-1], 4 )\n",
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB7IVL9xn0VG"
      },
      "source": [
        "def remove_values(df, percent):\n",
        "  total_data_to_delete = calculate_data_to_delete(df, percent)\n",
        "  return remove_n_values(df, total_data_to_delete)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGN-9JD5o6dK"
      },
      "source": [
        "df_iris_missing_values = remove_values(df_iris.iloc[:,:-1], percent=0.15 )\n",
        "df_iris_missing_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0B9_YqEppeq"
      },
      "source": [
        "df_iris_missing_values.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMI5UhSkp8b-"
      },
      "source": [
        "### 5.2 Remplazo valores perdidos.\n",
        "\n",
        "- Reemplazar por la media\n",
        "- Reemplazar por la mediana\n",
        "\n",
        "- Calcular error medio entre los conjuntos de datos original y con valores perdidos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbnenx7Zs4hG"
      },
      "source": [
        "# reemplazamos los valores por la media:\n",
        "df_iris_missing_values_filled_media = df_iris_missing_values.fillna(df_iris_missing_values.median())\n",
        "df_iris_missing_values_filled_media"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohICkiCz2f-8"
      },
      "source": [
        "#reemplazamos por la mediana de cada columna\n",
        "df_iris_missing_values_aux = df_iris_missing_values.dropna()\n",
        "\n",
        "dic_aux = dict(zip(list(df_iris_missing_values.columns), \n",
        "                        [statistics.median(df_iris_missing_values_aux.iloc[:,1]), \n",
        "                          statistics.median(df_iris_missing_values_aux.iloc[:,1]), \n",
        "                          statistics.median(df_iris_missing_values_aux.iloc[:,2]),\n",
        "                          statistics.median(df_iris_missing_values_aux.iloc[:,3])]))\n",
        "\n",
        "dic_aux\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qowGAk33w9D"
      },
      "source": [
        "df_iris_missing_values_filled_median = df_iris_missing_values.fillna(dic_aux)\n",
        "df_iris_missing_values_filled_median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KCTqyz87wGO"
      },
      "source": [
        "# calculamos el error\n",
        "df_err_iris_filled_median = (df_iris.iloc[:,:-1] - df_iris_missing_values_filled_median).abs().mean()\n",
        "df_err_iris_filled_median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B02c4Nbv_z6k"
      },
      "source": [
        "# calculamos el error\n",
        "df_err_iris_filled_media = (df_iris.iloc[:,:-1] - df_iris_missing_values_filled_media).abs().mean()\n",
        "df_err_iris_filled_media"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuRdOZx_AlYV"
      },
      "source": [
        "TODO Comentario:\n",
        "Aunque la diferencia es pequeña se percibe un menor error unado la media."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkx4_ahYAUWW"
      },
      "source": [
        "### 5.3 Normalizar los datos usando varios metodos\n",
        "\n",
        "- min_max\n",
        "- decimal\n",
        "- Desviación estandar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpFiJe-CAw7x"
      },
      "source": [
        "#min_max\n",
        "\n",
        "df_iris_min_max =  preprocessing.MinMaxScaler().fit_transform(df_iris.iloc[:,:-1])\n",
        "df_iris_min_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEPOppPLCSvD"
      },
      "source": [
        "df_iris_norm_std =  norm_desviacion_estandar(df_iris.iloc[:,:-1])\n",
        "df_iris_norm_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKuG9HLNCdwY"
      },
      "source": [
        "df_iris_norm_dec =  norm_dec(df_iris.iloc[:,:-1])\n",
        "df_iris_norm_dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltrqIW5qEJoQ"
      },
      "source": [
        "TODO discusión de los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0oMYpikWAJK"
      },
      "source": [
        "## 6.Realizar reducción de valores basado en la técnica de BIN con el mejor corte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PhzNyEQZUPc"
      },
      "source": [
        "Dado el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H9lzlZFWA8x"
      },
      "source": [
        "df_bin = pd.DataFrame(np.array([[1, 5.9, 3.4], \n",
        "                                [2, 2.1, 6.2], \n",
        "                                [1, 1.6, 2.8],\n",
        "                                [2, 6.8, 5.8],\n",
        "                                [1, 3.1, 3.1],\n",
        "                                [1, 8.3, 4.1],\n",
        "                                [2, 2.4, 5.0]]),\n",
        "                            columns=['I1', 'I2', 'I3'])\n",
        "\n",
        "df_bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_xYIddUWRiP"
      },
      "source": [
        "- Dimensión I2 usando la media como representantes de 2 BINS\n",
        "- Dimensión I3 usando el limite más cercano como representante de 2 BINS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL7GJO5S_20g"
      },
      "source": [
        "def init_list_bin_pos(serie, n_bins):\n",
        "  len = serie.shape[0]\n",
        "  bin_width = math.floor(len/n_bins)\n",
        "  result = []\n",
        "  value = 0\n",
        "  for x in range(0,n_bins):\n",
        "    value = value + bin_width\n",
        "    if x == n_bins - 1:\n",
        "      value = len\n",
        "    result.append(value)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8dBcGDvhtDg"
      },
      "source": [
        "#prueba init_list_bin_pos\n",
        "print(init_list_bin_pos(pd.Series([1,1,2,  3,3,4]), 2)) #[3, 6]\n",
        "print(init_list_bin_pos(pd.Series([1,1,2,  3,4,5, 5,6,8,9]), 3)) #[3, 6, 10]\n",
        "print(init_list_bin_pos(pd.Series([1,1, 2,3, 4,5,6]), 3)) #[2, 4, 7]\n",
        "print(init_list_bin_pos(pd.Series([1,1, 2,3, 4,5,5,6]), 3)) #[2, 4, 8]\n",
        "print(init_list_bin_pos(df_bin['I2'].sort_values(ignore_index=True), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GDOjVxFCkvX"
      },
      "source": [
        "def get_bin_pos(list_pos, i):\n",
        "  limit_s = list_pos[i]\n",
        "  if i>0:\n",
        "    limit_i = list_pos[i-1]\n",
        "  else:\n",
        "    limit_i = 0\n",
        "  return (limit_i, limit_s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfqBmfn96Xv0"
      },
      "source": [
        "print(get_bin_pos([4, 8, 10], i=0))\n",
        "print(get_bin_pos([4, 8, 10], i=1))\n",
        "print(get_bin_pos([4, 8, 10], i=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8LEaJOb6KcF"
      },
      "source": [
        "def get_bin_width(list_pos, i):\n",
        "  pos = get_bin_pos(list_pos, i)\n",
        "  return pos[1] - pos[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JBppFv06ofF"
      },
      "source": [
        "print(get_bin_width([4, 8, 10], i=0))\n",
        "print(get_bin_width([4, 8, 10], i=1))\n",
        "print(get_bin_width([4, 8, 10], i=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pMy7BjK2RGn"
      },
      "source": [
        "def get_bin_items(serie, list_pos, i):\n",
        "  pos = get_bin_pos(list_pos, i)\n",
        "  return serie.iloc[pos[0]:pos[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng76f9TZhVZE"
      },
      "source": [
        "#prueba get_bin_items\n",
        "print(get_bin_items(pd.DataFrame([1,1,2,  3,3,4]), list_pos=[3,6], i=0))\n",
        "print('----')\n",
        "print(get_bin_items(pd.DataFrame([1,1,2,3, 4,5,5,6, 8,9]), list_pos=[4, 8, 10], i=1))\n",
        "print('----')\n",
        "print(get_bin_items(pd.DataFrame([1,1,2,3, 4,5,5,6, 8,9]), list_pos=[4, 8, 10], i=2))\n",
        "print('----')\n",
        "print(get_bin_items(df_bin['I2'].sort_values(ignore_index=True), list_pos=[3, 7], i=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wxcgv5AGFwj"
      },
      "source": [
        "def move_items(serie, list_pos):\n",
        "  #se mueven items apartir del bin1 hacia el bin anterior\n",
        "  #si el item esta repetido, se mueven tantos como sea necesario\n",
        "  #Ej: {1,1,2,  3,3,4} -> {1,1,2,3,3,  4}:\n",
        "  new_list_pos = list_pos.copy()\n",
        "  len_serie = serie.shape[0] -1\n",
        "  last_bin = len(list_pos)-1\n",
        "  \n",
        "  for bin in range(0, last_bin):\n",
        "    #el último bin no mueve datos\n",
        "    if bin == last_bin:\n",
        "      break\n",
        "\n",
        "    limit_s = list_pos[bin]\n",
        "    #para garantizar que se ejecute al menos una vez\n",
        "    cont = True\n",
        "    while cont:\n",
        "      \n",
        "      limit_s_value_before = serie.iloc[limit_s]\n",
        "      if limit_s >= len_serie:\n",
        "          break\n",
        "      #movemos el limite una unidad hacia la derecha\n",
        "      limit_s = limit_s + 1\n",
        "      limit_s_value = serie.iloc[limit_s]\n",
        "      #print(f'En limite: {limit_s}, esta el valor: {limit_s_value}, el anterior {limit_s_value_before}')\n",
        "\n",
        "      cont = limit_s_value == limit_s_value_before\n",
        "      \n",
        "    new_list_pos[bin] = limit_s\n",
        "  return new_list_pos\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jozjmo31CAhI"
      },
      "source": [
        "print(move_items(df_bin['I2'].sort_values(ignore_index=True), list_pos=[3,7])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKB3QXoMtMM"
      },
      "source": [
        "# prueba move_items:\n",
        "print(move_items(pd.Series([1,1,2,  3,3,4]), list_pos=[3,6])) #> [5,6]\n",
        "print(move_items(pd.Series([1,1,2,  3,4, 5,5,6]), list_pos=[3,5,9])) #> [4,7,9]\n",
        "print(move_items(pd.Series([1,1,2,  3,4, 5,6,6]), list_pos=[3,5,9])) #> [4,6,9]\n",
        "print(move_items(pd.Series([1,1,2,  3,4,5, 6]), list_pos=[3,6,7])) #> [4,6,7]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBgpkN70gVQu"
      },
      "source": [
        "'''\n",
        "serie: pandas Series\n",
        "list_pos: listado de posiciones\n",
        "fn_rep: función que calcula la representación del bin, recibe una serie que con los datos del bin\n",
        "'''\n",
        "def get_repr(serie, list_pos, fn_rep):\n",
        "  n_bins = len(list_pos)\n",
        "  l = list_pos[n_bins-1]\n",
        "  result = []\n",
        "  for i in range(0, n_bins):\n",
        "    value = fn_rep(get_bin_items(serie, list_pos, i))\n",
        "    result = result + [value]*get_bin_width(list_pos, i)\n",
        "  return pd.Series(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJCsXjg2hFwZ"
      },
      "source": [
        "print(get_repr(serie=pd.Series([1,1,2,  3,4]), list_pos=[3,5], fn_rep=lambda serie: serie.mode().iloc[0])) # > [1 1 1 3 3 ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACuM34MoHJdr"
      },
      "source": [
        "def compute_error(serie, list_pos, fn_rep):\n",
        "  bin_repr = get_repr(serie, list_pos, fn_rep)\n",
        "  er = (serie - bin_repr).abs().sum()\n",
        "  return er,bin_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xb-qpddenvR"
      },
      "source": [
        "def serie_discretization(serie, fn_rep, n_bins=2):\n",
        "\n",
        "  # 1. Ordenar valores\n",
        "  serie_o = serie.sort_values(ignore_index=True)\n",
        "  #print(serie_o)\n",
        "\n",
        "  # 2. Asignar aproximadamente igual número de valores (V_i) a cada bin\n",
        "  # generamos la lista de limites superiores (indices) de los bins\n",
        "  bin_list_pos = init_list_bin_pos(serie_o, n_bins) \n",
        "\n",
        "  # 2.1. Calcular error\n",
        "  er,bin_repr = compute_error(serie_o, bin_list_pos, fn_rep)\n",
        "\n",
        "  cont = True\n",
        "  while cont:\n",
        "    #3. Mover al borde el elemento V_i al siguiente o previo\n",
        "    bin_list_pos_new = move_items(serie_o, bin_list_pos)\n",
        "\n",
        "    #3.1 Calcular error\n",
        "    er_new,bin_repr_new = compute_error(serie_o, bin_list_pos_new, fn_rep)\n",
        "    \n",
        "    if er_new < er:\n",
        "      #print(f'ER nuevo: {er_new} ER viejo: {er} continuamos ...')\n",
        "      bin_list_pos = bin_list_pos_new\n",
        "      er = er_new\n",
        "      bin_repr = bin_repr_new\n",
        "    else:\n",
        "      #print(f'ER nuevo: {er_new} ER viejo: {er} nos quedamos con el old.')\n",
        "      cont = False\n",
        "  return bin_repr, bin_list_pos, er"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8pSeMv_3lgg"
      },
      "source": [
        " #de la moda se selecciona el primero con iloc[0]\n",
        "serie_discretization(pd.Series([5, 1, 8, 2, 2, 2, 1, 8, 6, 9]),lambda bin: bin.mode().iloc[0],  3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK3yIqDf5wFu"
      },
      "source": [
        "serie_discretization(df_bin['I2'], lambda b: b.median()) #media"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSmhMGS78Vab"
      },
      "source": [
        "serie_discretization(df_bin['I2'], lambda b: b.iloc[0]) #limite mas cercano"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7l7agZa5-jV"
      },
      "source": [
        "serie_discretization(df_bin['I3'], lambda b: b.median()) #media"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlgNIL6b8gHb"
      },
      "source": [
        "serie_discretization(df_bin['I3'], lambda b: b.iloc[0]) #limite mas cercano"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1bI4DqTXQI_"
      },
      "source": [
        "## 7.Hacer el ranking de las dimensiones realizando comparación de medias y varianzas\n",
        "Dado el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veu8Bn8RXxUZ"
      },
      "source": [
        "df_ranking = pd.DataFrame(np.array([[2.5, 1.6, 5.9, 0], \n",
        "                                    [7.2, 4.3, 2.1, 1], \n",
        "                                    [3.4, 5.8, 1.6, 1],\n",
        "                                    [5.6, 3.6, 6.8, 0],\n",
        "                                    [4.8, 7.2, 3.1, 1],\n",
        "                                    [8.1, 4.9, 8.3, 0],\n",
        "                                    [6.3, 4.8, 2.4, 1]]),\n",
        "                                columns=['I1', 'I2', 'I3', 'C'])\n",
        "\n",
        "df_ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJXF2T9Ksjme"
      },
      "source": [
        "df_ranking.var()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy1XBekpq29L"
      },
      "source": [
        "df_ranking_C_0 = df_ranking.query('C == 0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O0IxO2grdeM"
      },
      "source": [
        "df_ranking_C_1 = df_ranking.query('C == 1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PgoGKLCtzTS"
      },
      "source": [
        "df_ranking_C_1.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5d1ocuIr3gr"
      },
      "source": [
        "'''\n",
        "A, B: pandas Series\n",
        "'''\n",
        "def compute_var_rank(A, B):\n",
        "  n1 = A.shape[0]\n",
        "  n2 = B.shape[0]\n",
        "\n",
        "  meanA = A.mean() \n",
        "  meanB = B.mean()\n",
        "  \n",
        "  SE = np.sqrt(A.var()/n1 + B.var()/n2)\n",
        "  return np.abs(meanA - meanB)/SE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyzZRYqTvI7Q"
      },
      "source": [
        "def ranking(df_C0, df_C1, b):\n",
        "  selected = {}\n",
        "  ignored = {}\n",
        "  for col in list(df_C0.columns):\n",
        "    rank = compute_var_rank(df_C0[[col]], df_C1[[col]])\n",
        "    rank_value = rank.iloc[0]\n",
        "    if rank_value > b:\n",
        "      selected[col] = rank_value\n",
        "    else:\n",
        "      ignored[col] = rank_value\n",
        "\n",
        "  return selected, ignored"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7EbXFqttKj8"
      },
      "source": [
        "compute_var_rank(df_ranking_C_0[['I1']], df_ranking_C_1[['I1']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEp3-I9kAATj"
      },
      "source": [
        "# para hacer el ranking de los datos\n",
        "ranking(df_ranking_C_0, df_ranking_C_1, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ih6tDC7YVgO"
      },
      "source": [
        "## 8.Dado el conjunto de datos X, donde X1 y X2 son dimensiones numericas, X3 y X4 son dimensiones con datos categoricos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGGrJOlfYfJl"
      },
      "source": [
        "df_hetero = pd.DataFrame(np.array([[2.7, 3.4, 1, 'A'], \n",
        "                                    [3.1, 6.2, 2, 'A'], \n",
        "                                    [4.5, 2.8, 1, 'B'],\n",
        "                                    [5.3, 5.8, 2, 'B'],\n",
        "                                    [6.6, 3.1, 1, 'A'],\n",
        "                                    [5.0, 4.1, 2, 'B']]),\n",
        "                                columns=['X1', 'X2', 'X3', 'X4'])\n",
        "\n",
        "df_hetero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcBoHtEwvI5J"
      },
      "source": [
        "df_hetero['X1'] = pd.to_numeric(df_hetero['X1'])\n",
        "df_hetero['X2'] = pd.to_numeric(df_hetero['X2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dc21fMdFF6i"
      },
      "source": [
        "cols_groups = { 'cat': ['X3', 'X4'], 'num': ['X1', 'X2']}\n",
        "cols_groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etL8hmCFAntk"
      },
      "source": [
        "### 8.1 Implementación imperativa inicial (poco eficiente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV0whqhEmO9c"
      },
      "source": [
        "def make_pair_df(X, N, i):\n",
        "  j = i+1\n",
        "  #Se replica la fila actual\n",
        "  X_i_a = [X.iloc[i,:].to_numpy()]*(N-j)\n",
        "  \n",
        "  X_j_a = X.iloc[j:,:].to_numpy()\n",
        "\n",
        "  # Paso adicional para resetear los indices\n",
        "  X_i = pd.DataFrame(X_i_a, columns=X.columns)\n",
        "  X_j = pd.DataFrame(X_j_a, columns=X.columns)\n",
        "  return X_i, X_j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g8vQcSamo5L"
      },
      "source": [
        "make_pair_df(df_hetero, df_hetero.shape[0], 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bUAJ0Xr_JAy"
      },
      "source": [
        "#util para manejar columnas\n",
        "def add_sufix(p_list, suffix):\n",
        "  return list(map(lambda i: i+'_'+suffix, p_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwTgm9ipBXAv"
      },
      "source": [
        "#util para manejar columnas\n",
        "def build_column_names(p_list):\n",
        "  return (add_sufix(p_list, '1'), add_sufix(p_list, '2'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDUDjT4hCqqF"
      },
      "source": [
        "#util para la reducción de dimensiones conocer\n",
        "#a que grupo pertence (Categórico/Numérico)\n",
        "def intersect_cols(cols1, cols2):\n",
        "  return [value for value in cols1 if value in cols2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLfu7z3zHYKc"
      },
      "source": [
        "cols_u = ['X1','X2', 'X3']\n",
        "cols_cat = ['X3', 'X4']\n",
        "intersect_cols(cols_u, cols_cat) # > ['X3']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF7uSgxNkjg5"
      },
      "source": [
        "def custom_merge(df1, df2):\n",
        "  #closure util para mover columnas\n",
        "  def select_and_rename(df_from, original_names, new_names):\n",
        "    return df_from.loc[:,original_names].rename(columns=dict(zip(original_names,new_names)))\n",
        "\n",
        "  #juntamos los objetos en el mismo dataframe\n",
        "  cols = df1.columns.to_list()\n",
        "  cols1,cols2 = build_column_names(cols)\n",
        "\n",
        "  df_merged = select_and_rename(df1, cols, cols1)\n",
        "  df_merged.loc[:,cols2] = select_and_rename(df2, cols, cols2)\n",
        "\n",
        "  return df_merged\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IhT4XgynZlA"
      },
      "source": [
        "df1, df2 = make_pair_df(df_hetero, df_hetero.shape[0], 0)\n",
        "df_merged = custom_merge(df1, df2)\n",
        "df_merged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAa7iRjgqSLu"
      },
      "source": [
        "def calc_similarity_categoric(row, col1, col2):\n",
        "  l = len(col1)\n",
        "  if l == 0:\n",
        "    return 0\n",
        "  acc = 0.\n",
        "  for i in range(0,l):\n",
        "    #Para los categóricos simplemente se compararán\n",
        "    acc = acc + (row[col1[i]] == row[col2[i]])\n",
        "  return acc/l\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyhXUNyFLT92"
      },
      "source": [
        "print(calc_similarity_categoric(df_merged.iloc[0,:], [],  [])) #> 0\n",
        "print(calc_similarity_categoric(df_merged.iloc[0,:], ['X3_1'],  ['X3_2'])) #> 0\n",
        "print(calc_similarity_categoric(df_merged.iloc[0,:], ['X3_1', 'X4_1'],  ['X3_2', 'X4_2'])) #> 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amm6gkiXr91Z"
      },
      "source": [
        "def calc_similarity_numeric(row, col1, col2):\n",
        "  #Para los campos númericos se usara coseno\n",
        "  if not col1 or not col2:\n",
        "    return 0\n",
        "  X = row[col1]\n",
        "  Y = row[col2] \n",
        "  normXY = np.linalg.norm(X.values)*np.linalg.norm(Y.values)\n",
        "  return  1. - np.dot(X,Y)/normXY\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDMiL9u3KtSs"
      },
      "source": [
        "print(calc_similarity_numeric(df_merged.iloc[0,:], [], [])) # 0\n",
        "print(calc_similarity_numeric(df_merged.iloc[0,:], ['X1_1'], ['X1_2']))\n",
        "print(calc_similarity_numeric(df_merged.iloc[0,:], ['X1_1', 'X2_1'], ['X1_2', 'X2_2'])) #0.021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhAMHT9VlWFV"
      },
      "source": [
        "\n",
        "def build_similarity_by_row(cols, cols_groups):\n",
        "\n",
        "  cnum1,cnum2 = build_column_names(intersect_cols(cols, cols_groups['num']))\n",
        "  ccat1,ccat2 = build_column_names(intersect_cols(cols, cols_groups['cat']))\n",
        "\n",
        "  def f(row):\n",
        "    s_num = calc_similarity_numeric(row, cnum1, cnum2)\n",
        "    s_cat = calc_similarity_categoric(row, ccat1,  ccat2)\n",
        "    s = (s_num + s_cat)/2\n",
        "    return s\n",
        "  \n",
        "  return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Z4zNkMlZ7d"
      },
      "source": [
        "calc_similarity_by_row = build_similarity_by_row(list(df_merged.columns), cols_groups=cols_groups)\n",
        "calc_similarity_by_row(df_merged.iloc[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAGqagJBwL53"
      },
      "source": [
        "def build_similarity_vector(X, cols_groups):\n",
        "  N = X.shape[0]\n",
        "  s_aux = []\n",
        "\n",
        "  #se crea closure para aplicar en cada row\n",
        "  calc_similarity_by_row = build_similarity_by_row(list(X.columns), cols_groups)\n",
        "\n",
        "  # se crea el vector de similaridades\n",
        "  for i in range(0,N-1):\n",
        "    X_i, X_j = make_pair_df(X, N, i)\n",
        "    s_aux.append(custom_merge(X_i, X_j).apply(calc_similarity_by_row, axis=1))\n",
        "  \n",
        "  return pd.concat(s_aux)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUAWzGkfxWZO"
      },
      "source": [
        "S_ij = build_similarity_vector(df_hetero[['X3','X4', 'X2']], cols_groups=cols_groups)\n",
        "S_ij"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PLD1tUkxw1v"
      },
      "source": [
        "#Este método tiene muy bajo rendimiento, mejorar\n",
        "def calc_entropy(X, cols_groups):\n",
        "  S_ij = build_similarity_vector(X, cols_groups)\n",
        "  S_ij_ = 1 - S_ij\n",
        "\n",
        "  # se calcula la entropía\n",
        "  return -(S_ij*np.log(S_ij) + S_ij_*np.log(S_ij_)).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXNQVwVCyOMa"
      },
      "source": [
        "calc_entropy(df_hetero[['X3','X4', 'X2']], cols_groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhc4Uq0kBIVy"
      },
      "source": [
        "### 8.2 Implementación usando SciPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHUM2IxVfe8k"
      },
      "source": [
        "# función de utilidad para convertir datos categoricos a númericos\n",
        "def cat2num(df, colnames):\n",
        "  dict_le = {}\n",
        "  result = pd.DataFrame()\n",
        "  for col in colnames:\n",
        "    dict_le[col] = preprocessing.LabelEncoder()\n",
        "    dict_le[col].fit(df[col])\n",
        "    result[col] = dict_le[col].transform(df[col])\n",
        "  return result, dict_le"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKKDItfTBbTe"
      },
      "source": [
        "#función que calcula la similitud\n",
        "# para los datos númericos se usa coseno y para los categóricos se una Hamming\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "def calc_entropy2(df, cols_groups):\n",
        "\n",
        "  # obtenemos las columnas del dataframe\n",
        "  cols = df.columns\n",
        "  rows = df.shape[0]\n",
        "\n",
        "  #closure útilidad\n",
        "  def intersect_cols(cols1, cols2):\n",
        "    return [value for value in cols1 if value in cols2]\n",
        "\n",
        "  #closure útilidad\n",
        "  def df_or_zeros(p_df, fn):\n",
        "    if p_df.shape[1] > 0:\n",
        "      return fn(p_df)\n",
        "    else: \n",
        "      return np.zeros(shape=(rows, rows))\n",
        "\n",
        "  # seleccionamos las columnas númericas\n",
        "  cols_num = intersect_cols(cols, cols_groups['num'])\n",
        "  # seleccionamos las columnas categóricas\n",
        "  cols_cat = intersect_cols(cols, cols_groups['cat'])\n",
        "\n",
        "  df_num = df[cols_num]\n",
        "  #df_num = df_or_zeros(df, cols_num, lambda df, c: df[c]) \n",
        "\n",
        "  df_cat,_ = cat2num(df, cols_cat)\n",
        "  #df_cat = df_or_zeros(df, cols_cat, lambda df, c: cat2num(df, c)[0])\n",
        "\n",
        "  #calculamos las distancias por separado\n",
        "  d_num = df_or_zeros(df_num, lambda d: distance.cdist(d, d, 'cosine'))\n",
        "  #print(d_num)\n",
        "\n",
        "  d_cat = df_or_zeros(df_cat, lambda d: distance.cdist(d, d, 'hamming'))\n",
        "  #print(d_cat)\n",
        "  #se calcula el factor para el promedio ponderado\n",
        "  factor = np.array([len(cols_num), len(cols_cat)])/len(cols)\n",
        "\n",
        "  # ponderado entre los campos númericos y categóricos\n",
        "  dis = (factor[0]*d_num + factor[1]*d_cat)/2\n",
        "  #print(dis)\n",
        "\n",
        "  # transformamos a similaridad\n",
        "  sim = 1. - dis\n",
        "  sim_ = dis # 1 - sim, no lo calculamos otra vez\n",
        "\n",
        "  # se calcula la entropía\n",
        "  e = (entropy(sim) + entropy(sim_)).sum()\n",
        "  return e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDZthATHLR0k"
      },
      "source": [
        "calc_entropy2(df_hetero[['X4', 'X3']], cols_groups )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYBYgFh-ZPNq"
      },
      "source": [
        "- Aplicar el método se selección de características basado en la entropía para reducir una dimensión (mostrar pasos).\n",
        "- Implementar un programa para realizar el “ranking” de dimensiones usando entropía."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dodVKaybwXnv"
      },
      "source": [
        "# Implementación algoritmo:\n",
        "\n",
        "# 1. Tomamos todo el conjunto de datos\n",
        "F = df_hetero\n",
        "ranking = []\n",
        "\n",
        "cont = True\n",
        "while cont:\n",
        "  # 2. Calculamos la entropía\n",
        "  E_F = calc_entropy2(F, cols_groups) #calc_entropy(F, cols_groups)\n",
        "\n",
        "  # 3. Por cada dimensión (Columna)\n",
        "  cols = list(F.columns)\n",
        "  #condición de parada: hasta que solo quede una dimensión\n",
        "  if len(cols) == 1:\n",
        "    ranking.append(cols[0])\n",
        "    break\n",
        "  \n",
        "  E_F_list = []\n",
        "  E_F_dict = {}\n",
        "\n",
        "  for col in cols:\n",
        "\n",
        "    #eliminamos el atributo\n",
        "    cols_aux = cols.copy()\n",
        "    cols_aux.remove(col)\n",
        "    Ff = F[cols_aux]\n",
        "    \n",
        "    E_Ff = calc_entropy2(Ff, cols_groups)  #calc_entropy(F, cols_groups)\n",
        "    \n",
        "    diff = E_F - E_Ff\n",
        "\n",
        "    E_F_list.append(diff)\n",
        "    E_F_dict[diff] = col\n",
        "\n",
        "  # luego de calcular todos las entropias, buscamos la diferencia minima\n",
        "  min_E_F = min(E_F_list)\n",
        "  col = E_F_dict[min_E_F]\n",
        "\n",
        "  cols.remove(col)\n",
        "  F = F[cols]\n",
        "  ranking.append(col)\n",
        "\n",
        "  print(E_F_dict)\n",
        "ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBouzGgDZbl5"
      },
      "source": [
        "## 9.Al conjunto de datos Adult del repositorio de Machine learning:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWptAah3ZcsX"
      },
      "source": [
        "Este conjunto de datos corresponde a datos del censo de USA de 1994 (https://archive.ics.uci.edu/ml/index.php)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4ZEjPNHZz6e"
      },
      "source": [
        "# Carga de los datos\n",
        "#uploaded = files.upload()\n",
        "#df_adult = pd.read_csv(io.BytesIO(uploaded['adult.data']), sep=',', names=['age', 'workclass', 'fnlwgt', 'education','education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income' ])\n",
        "df_adult = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', sep=',', names=['age', 'workclass', 'fnlwgt', 'education','education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income' ])\n",
        "\n",
        "df_adult.shape\n",
        "df_adult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVksQajRZlF_"
      },
      "source": [
        "### 9.1Convertir todos los atributos numéricos a categóricos utilizando dos estrategias diferentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMRiv8GrZkHX"
      },
      "source": [
        "#seleccionamos las columnas que son numericas\n",
        "cols_num = ['age', 'hours-per-week', 'education-num', 'fnlwgt', 'capital-gain', 'capital-loss' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9lDREjSntFo"
      },
      "source": [
        "def serie_discretization_search_bins(serie, fn_rep, stop_diff=0.3, max_bins = 10):\n",
        "   #TODO implementar un algoritmo de busqueda mas eficiente\n",
        "  # calculamos los er\n",
        "  \n",
        "  n_bins = 2\n",
        "  serie_disc, _, er = serie_discretization(serie, fn_rep, n_bins)\n",
        "  diff = 1\n",
        "  cont = True\n",
        "  while cont:\n",
        "    serie_disc_new, _, er_new = serie_discretization(serie, fn_rep, n_bins)\n",
        "    diff = er_new/er\n",
        "    print(f'diff er: {diff}')\n",
        "    if diff <= stop_diff or n_bins >= max_bins:\n",
        "      cont = False\n",
        "      serie_disc = serie_disc_new\n",
        "      er = er_new\n",
        "    else:\n",
        "      n_bins = n_bins +1\n",
        "\n",
        "  print(f'mejor encontrado -> n_bins: {n_bins}, er: {er}')\n",
        "  return serie_disc, er"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM9trL6B0dU-"
      },
      "source": [
        "df_adult_num = df_adult[cols_num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX689znet-rs"
      },
      "source": [
        "serie_discretization_search_bins(df_adult_num['age'], lambda b: b.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSfLl89h0UWX"
      },
      "source": [
        "def df_discretization(df, col_names, fn_rep, stop_diff=0.5, max_bins = 10):\n",
        "  result = df\n",
        "  for col in col_names:\n",
        "    print(f'Se inicia proceso de discretización para la col: {col}')\n",
        "    result = result.sort_values(by=[col], ignore_index=True)\n",
        "    serie_disc, _ = serie_discretization_search_bins(result[col], fn_rep, stop_diff, max_bins )\n",
        "    result[col] = serie_disc\n",
        "\n",
        "  return result\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa1GT0HFcdbW"
      },
      "source": [
        "df_adult_disc = df_discretization(df_adult, ['age', 'hours-per-week', 'education-num', 'fnlwgt'] , lambda b: b.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACh7B6t8iUF"
      },
      "source": [
        "df_adult_disc = df_discretization(df_adult_disc, ['capital-gain', 'capital-loss'] , lambda b: b.iloc[0],  max_bins = 2) #limite mas cercano"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpF0ig0A88Hq"
      },
      "source": [
        "df_adult_disc.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVF3bNXf9oyh"
      },
      "source": [
        "\n",
        "### 9.2Transformar el conjunto de datos de manera que todos los atributos sean numéricos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHskVpP79khB"
      },
      "source": [
        "#seleccionamos las columnas que son categóricas\n",
        "cols_cat = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'income' ]\n",
        "\n",
        "df_adult['workclass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfOLxxljJjT5"
      },
      "source": [
        "#Para transformar datos categóricos a númericos se usará\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmnpslexJ0sA"
      },
      "source": [
        "s = le.fit(df_adult['sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMJ8SIoeJ_t2"
      },
      "source": [
        "s.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8i723sSKFxR"
      },
      "source": [
        "le.transform(df_adult['sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj80Ga5tKMKq"
      },
      "source": [
        "def cat2num(df, colnames):\n",
        "  dict_le = {}\n",
        "  result = pd.DataFrame()\n",
        "  for col in colnames:\n",
        "    dict_le[col] = preprocessing.LabelEncoder()\n",
        "    dict_le[col].fit(df[col])\n",
        "    result[col] = dict_le[col].transform(df[col])\n",
        "  return result, dict_le\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skUSDGulLlmS"
      },
      "source": [
        "cat2num(df_adult, cols_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbCU_qgwU8J-"
      },
      "source": [
        "## 10.Aplicación PCA\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_884RFRZh7N"
      },
      "source": [
        "El conjunto de datos seleccionado es [Smartphone Dataset for Human Activity Recognition (HAR) in Ambient Assisted Living (AAL) Data Set\n",
        "](https://archive.ics.uci.edu/ml/datasets/Smartphone+Dataset+for+Human+Activity+Recognition+%28HAR%29+in+Ambient+Assisted+Living+%28AAL%29) \n",
        "\n",
        "\n",
        "Descripción del conjunto de datos:\n",
        "\n",
        "El conjunto contiene información de medidas de sensores para reconocer la posición de la actividad humana. El conjunto muestra los valores de los sensores por cada eje (X, Y, Z) y esta aumentado por variables calculadas como correlaciones entre ejes, entropía, energía etc llegando a un total de 561 columnas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1LQFoDOglzS"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00364/dataset_uci.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dy3HW1Mg3D1"
      },
      "source": [
        "!unzip dataset_uci.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtyChm3Yhe0Q"
      },
      "source": [
        "HOME_DATA_SET = './dataset_uci'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scIiH9cChPru"
      },
      "source": [
        "# Abrimos el features.txt para explorar su contenido\n",
        "headers = [] \n",
        "with open(HOME_DATA_SET + '/features.txt', 'r') as header_txt:\n",
        "    headers = [line.strip() for line in header_txt.readlines()]\n",
        "headers \n",
        "\n",
        "#headers = all_data[0].split(',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OMEksrjiRUr"
      },
      "source": [
        "har_aal_filename = HOME_DATA_SET + '/final_X_test.txt'\n",
        "df_har_aal = pd.read_csv(har_aal_filename, ',', names=headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg_pG80kiiS2"
      },
      "source": [
        "df_har_aal.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWC1J6dCEhrl"
      },
      "source": [
        "**PCA**\n",
        "\n",
        "Es un método clásico para reducir el número de atributos en los datos proyectando los datos desde su espacio original de alta dimensión en un espacio de menor dimensión. Los nuevos atributos (también conocidos como componentes) creados por PCA tienen las siguientes propiedades: \n",
        "\n",
        "- son combinaciones lineales del atributo original\n",
        "- son ortogonales (perpendiculares) entre sí\n",
        "- capturan la máxima cantidad de variación en los datos.\n",
        "\n",
        "Importante:\n",
        "\n",
        "_No es una técnica de selección de características_. Más bien, es una técnica de combinación de características. Porque cada PC es una combinación aditiva ponderada de todas las columnas del conjunto de datos original. Más sobre esto cuando lo implemente en la siguiente sección.\n",
        "\n",
        "Para esta sección se utilizó el tutorial:\n",
        "\n",
        "https://www.machinelearningplus.com/machine-learning/principal-components-analysis-pca-better-explained/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydTBeNL-Enl3"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# PCA\n",
        "pca = PCA()\n",
        "df_pca = pca.fit_transform(X=df_har_aal)\n",
        "\n",
        "# Store as dataframe and print\n",
        "df_pca = pd.DataFrame(df_pca)\n",
        "print(df_pca.shape)  #> (3147, 784)\n",
        "df_pca.round(2).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG42ny6nFZVL"
      },
      "source": [
        "Importante:\n",
        "\n",
        "- El dataframe resultante contiene los componentes principales (PC) y tiene las mismas dimensiones del conjunto original.\n",
        "\n",
        "- *pca.components_* este objeto contiene los pesos:\n",
        " - cada PC es el producto punto entre los pesos y los datos autoescalados.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swtOfYeW_XRW"
      },
      "source": [
        "# para calcular cuantos PC son necesarios, teniendo en cuenta que 90% estaría bien\n",
        "variance_exp_cumsum = pca.explained_variance_ratio_.cumsum().round(2)\n",
        "max_pcs = variance_exp_cumsum[variance_exp_cumsum <= 0.9].shape[0]\n",
        "max_pcs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEL09xYrFZGG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1,1,figsize=(16,7), dpi=100)\n",
        "plt.plot(variance_exp_cumsum, color='firebrick')\n",
        "plt.vlines(x = max_pcs, ymin = variance_exp_cumsum[0], ymax =1.0, colors = 'purple') \n",
        "plt.title('Varianza acumulada%', fontsize=18)\n",
        "plt.xlabel('# de PCs', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzlNTNo9_n89"
      },
      "source": [
        "En conclusión aunque los PC no tienen una significado propio asociada con los datos, si que es una herramienta útil para reducir considerablemente las dimensiones, en este caso, se paso de 561 a solo 41, implica una reducción casi del 93%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwJMNcWONvvB"
      },
      "source": [
        "## 11.Análisis valores perdidos conjunto de datos Labour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaQZpl6f1KN7"
      },
      "source": [
        "En este punto se realizará un procedimiento similar al realizado en el punto 5.\n",
        "\n",
        "Para evitar el procesamiento manual, se carga desde la fuente: https://raw.githubusercontent.com/renatopp/arff-datasets/master/classification/labor.arff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve3QMr-h1pF4"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/renatopp/arff-datasets/master/classification/labor.arff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QciQSuJF11FB"
      },
      "source": [
        "from scipy.io import arff\n",
        "from io import StringIO\n",
        "\n",
        "data, meta = arff.loadarff('labor.arff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpDyCBV52swk"
      },
      "source": [
        "df_labor = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSt049yO2uOk"
      },
      "source": [
        "df_labor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGiApHk13t_7"
      },
      "source": [
        "# Calculamos el porcentaje de valores perdidos por atributo\n",
        "df_labor_missing_percent = df_labor.isna().sum()/df_labor.shape[0]*100\n",
        "df_labor_missing_percent[df_labor_missing_percent > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQqwEGQ84XLl"
      },
      "source": [
        "Vamos a revisar lo que pasa con los datos de:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Mz20l8-kM5"
      },
      "source": [
        "def imprimir_estadisticos(df):\n",
        "  df_dropna = df.dropna()\n",
        "  print(f'Media:   {df_dropna.mean()}')\n",
        "  print(f'Mediana: {df_dropna.median()}')\n",
        "  print(f'std:     {df_dropna.std()}')\n",
        "  print('---------')\n",
        "  print(f'moda: {df_dropna.mode()}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EipZQhZQ4qUY"
      },
      "source": [
        "# standby-pay:                  84.2% de datos faltantes\n",
        "imprimir_estadisticos(df_labor[['standby-pay']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZW0Yvb_6SZy"
      },
      "source": [
        "# wage-increase-third-year:     73.7%\n",
        "imprimir_estadisticos(df_labor[['wage-increase-third-year']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5N4Oy7-73CH"
      },
      "source": [
        "Para el caso de **standby-pay** y **wage-increase-third-year** y aunque depende del tipo de tarea a realizar con el conjunto de datos lo mas sensato sería quitarlos ya que al faltar mas del 70% de los datos no aportan información.  Tratar de reemplazarlos puede causar un riesgo de sesgo dentro del conjunto.  \n",
        "\n",
        "Aúnque las medidas de centralidad **wage-increase-third-year** son mas o menos cercanas y la dispersión no es muy alta, aún consideraría quitarlo por la falta de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHaZdwuk7CH8"
      },
      "source": [
        "# shift-differential:           45.614035\n",
        "imprimir_estadisticos(df_labor[['shift-differential']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDp998b-7sIt"
      },
      "source": [
        "**shift-differential** tiene menos de la mitad de los datos y unas medidas de centralidad alejadas entre si ademas de una dispersión relativamente alta, por lo que en este caso se podria realizar una tarea de regresión a partir de los datos existentes, para estimar el 55% de datos faltantes.\n",
        "\n",
        "Ahora se analizarán los atributos con menos del 20% de datos faltantes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zbGmey7-Jg1"
      },
      "source": [
        "# wage-increase-second-year    19.298246\n",
        "imprimir_estadisticos(df_labor[['wage-increase-second-year']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySI7mIcS_3dS"
      },
      "source": [
        "En este caso, la despersión es de un tamaño razonable y las medidas de centralidad son muy cercanas entre sí. \n",
        "\n",
        "Se revisa como cambían las estadisticas, reemplazando los valores faltantes por '4.0':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lnPG5HMAI31"
      },
      "source": [
        "df_labor_wage_increase_second_year = df_labor[['wage-increase-second-year']].fillna(4.0)\n",
        "imprimir_estadisticos(df_labor_wage_increase_second_year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RV65Za7_iXy"
      },
      "source": [
        "# working-hours                10.526316\n",
        "imprimir_estadisticos(df_labor[['working-hours']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTO2eQQAvSG"
      },
      "source": [
        "En el caso de **working-hours** tenemos dos opciones, reemplazar por la mediana o la moda y escoger el que cambie en menor porcentaje los estadisticos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un51zhRMAuq_"
      },
      "source": [
        "print('Usando la mediana: ')\n",
        "imprimir_estadisticos(df_labor[['working-hours']].fillna(38.0))\n",
        "print('Usando la moda: ')\n",
        "imprimir_estadisticos(df_labor[['working-hours']].fillna(40.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOrD8_djBj4t"
      },
      "source": [
        "En este caso es mejor usar la mediana = 38.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCphqkCj_xK0"
      },
      "source": [
        "# statutory-holidays            7.017544\n",
        "imprimir_estadisticos(df_labor[['statutory-holidays']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAjXE7oYByOl"
      },
      "source": [
        "Finalmente para **'statutory-holidays'** no hay mucha diferencia entre el uso de la media y la mediana, preferiblemente la mediana para evitar agregar datos atípicos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HTctsYpBv0m"
      },
      "source": [
        "print('Usando la mediana: ')\n",
        "imprimir_estadisticos(df_labor[['statutory-holidays']].fillna(11.0))\n",
        "print('Usando la media: ')\n",
        "imprimir_estadisticos(df_labor[['statutory-holidays']].fillna(11.09))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}